{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOp/NQVL6o6vDM9G/ZLlQTG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RiccardoRubini93/2D_NS_inc/blob/master/Test01_load_into.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADXaJzv2KSfQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "from google.cloud import bigquery\n",
        "import numpy as np\n",
        "from google.api_core import page_iterator\n",
        "from google.cloud import storage\n",
        "import os\n",
        "#import datetime\n",
        "from datetime import datetime, date, timedelta\n",
        "from datetime import timezone\n",
        "import pandas_gbq as pdbq\n",
        "import re\n",
        "\n",
        "\n",
        "def convert_datetime_to_string(data):\n",
        "  return data.strftime('%Y-%d-%m | %H:%M:%S')\n",
        "\n",
        "#next step is to add the extra columns\n",
        "\n",
        "def bucket_metadata(bucket_name,project_id):\n",
        "    \"\"\"Prints out a bucket's metadata.\"\"\"\n",
        "  \n",
        "    storage_client = storage.Client(project_id)\n",
        "    bucket = storage_client.get_bucket(bucket_name)\n",
        "\n",
        "    data                  = {}\n",
        "    data['bucket_id']     = bucket.id\n",
        "    data['bucket_name']          = bucket.name\n",
        "    data['bucket_storage_class'] = bucket.storage_class\n",
        "    data['bucket_location']      = bucket.location\n",
        "    data['bucket_location_type'] = bucket.location_type\n",
        "    data['bucket_created'] = str(bucket.time_created)\n",
        "\n",
        "    return data\n",
        "\n",
        "def file_path_policy_respected_check(file_path):\n",
        "    #function that checks whether the path policy of a certain bucket is respected or not\n",
        "    #each path should be in the following format \n",
        "    path_chuncks = file_path.split('/')[2:-1]\n",
        "    #print(path_chuncks)\n",
        "    if len(path_chuncks) == 5 : policy_respected = True\n",
        "    else : policy_respected = False\n",
        "\n",
        "    return policy_respected\n",
        "\n",
        "def _item_to_value(iterator, item):\n",
        "    return item\n",
        "\n",
        "def list_directories(bucket_name, prefix):\n",
        "\n",
        "    if prefix and not prefix.endswith('/'):\n",
        "        prefix += '/'\n",
        "\n",
        "    extra_params = {\n",
        "        \"projection\": \"noAcl\",\n",
        "        \"prefix\": prefix,\n",
        "        \"delimiter\": '/'\n",
        "    }\n",
        "\n",
        "    gcs = storage.Client()\n",
        "\n",
        "    path = \"/b/\" + bucket_name + \"/o\"\n",
        "\n",
        "    iterator = page_iterator.HTTPIterator(\n",
        "        client=gcs,\n",
        "        api_request=gcs._connection.api_request,\n",
        "        path=path,\n",
        "        items_key='prefixes',\n",
        "        item_to_value=_item_to_value,\n",
        "        extra_params=extra_params,\n",
        "    )\n",
        "\n",
        "    return [x for x in iterator]\n",
        "\n",
        "#definitions of helpers functions\n",
        "\n",
        "def file_path_pipeline_code_distinct_versions_check(input_path):\n",
        "  if (input_path [-1])!='/':\n",
        "    input_path+='/'\n",
        "    \n",
        "  bucket = input_path.split('/')[2]\n",
        "  #print(bucket)\n",
        "  prefix  = input_path.split(bucket)[1][1:-4]\n",
        "  #print(prefix)\n",
        "\n",
        "  #build a list containing path and corresponding sizes\n",
        "\n",
        "  versions_sizes = []\n",
        "\n",
        "  #print(list_directories(bucket,prefix))\n",
        "\n",
        "  for directory in list_directories(bucket,prefix):\n",
        "\n",
        "    path = \"gs://\"+ bucket + \"/\" +  directory \n",
        "    #print(path)\n",
        "    size = os.popen(\"gsutil du -s \" + str(path)).read().split()\n",
        "    #print(\"Size : \" + str(size))\n",
        "\n",
        "    versions_sizes.append(size)\n",
        "  return versions_sizes\n",
        "\n",
        "def get_file_number_in_landing_zone_path(input_path):\n",
        "\n",
        "    project_id = 'advanced-analytics-278408'\n",
        "    client = storage.Client(project_id)\n",
        "\n",
        "    bucket = input_path.split('/')[2]\n",
        "    prefix  = input_path.split(bucket)[1][1:-1]\n",
        "\n",
        "    count = 0\n",
        "\n",
        "    for blob in client.list_blobs(bucket,prefix=prefix):\n",
        "      count +=1\n",
        "\n",
        "    #print(\"Landing zone path : \" + str(input_path) )\n",
        "    print(\"File number : \" + str(count))\n",
        "\n",
        "    return count\n",
        "\n",
        "def get_file_size_in_landing_zone_path(input_path):\n",
        "\n",
        "    bucket = input_path.split('/')[2]\n",
        "    #print(bucket)\n",
        "    prefix  = input_path.split(bucket)[1][1:-4]\n",
        "    #print(prefix)\n",
        "\n",
        "    # corresponding sizes\n",
        "\n",
        "    path = \"gs://\"+ bucket + \"/\" +  prefix \n",
        "    #print(path)\n",
        "    size = os.popen(\"gsutil du -s \" + str(path)).read().split()[0]\n",
        "    print(\"Size : \" + str(size))\n",
        "  \n",
        "    return round(int(size) / 1024 ** 3, 2)\n",
        "\n",
        "def get_oldest_and_newest_file(input_path,project_id):\n",
        "\n",
        "  client = storage.Client(project_id)\n",
        "\n",
        "  bucket_name = input_path.split('/')[2]\n",
        "  prefix =      input_path.split(bucket_name)[1][1:-1]\n",
        "\n",
        "  #generate a list of blobs\n",
        "\n",
        "  blobs = client.list_blobs(bucket_name,prefix=prefix)\n",
        "\n",
        "  blobs = [(blob, blob.updated) for blob in client.list_blobs(bucket_name,prefix=prefix)]\n",
        "  #print(list(client.list_blobs(bucket_name,prefix = prefix)))\n",
        "  \n",
        "  \n",
        "  #sort blobs by update date\n",
        "  if len(blobs)>0:\n",
        "    newest  = sorted(blobs, key=lambda tup: tup[1])[-1] \n",
        "    oldest  = sorted(blobs, key=lambda tup: tup[1])[0]\n",
        "  else:\n",
        "    newest='No files found'\n",
        "    oldest='No files found'\n",
        "\n",
        "  return oldest,newest\n",
        "\n",
        "def is_file_policy_respected(input_path,project_id,bucket,prefix):\n",
        "\n",
        "  client = storage.Client(project_id)\n",
        "\n",
        "  name_is_conformal = True\n",
        "\n",
        "  print(\"Warning message: Check that the policy of the file is respected\")\n",
        "  #print(\"Bucket : \" + str(bucket))\n",
        "  #print(\"Prefix : \" + str(prefix))\n",
        "\n",
        "  for blob in client.list_blobs(bucket,prefix=prefix):\n",
        "\n",
        "    if len((blob.name.split('/')[-1]).split('__')) != 4 : \n",
        "      print(\"Warning message: Found not conformal file. The file that does not respect the policy is: \" + blob.name)\n",
        "      #print(blob.name)\n",
        "      name_is_conformal = False\n",
        "      break\n",
        "  \n",
        "  return name_is_conformal\n",
        "\n",
        "def extract_file_info(input_path,project_id,bucket):\n",
        "\n",
        "  client = storage.Client(project_id)\n",
        "\n",
        "  #divide la stringa input_path per estrarre il bucket_name e il prefisso per filtrare i BLOB.\n",
        "  bucket_name = input_path.split('/')[2]\n",
        "  #print(bucket_name)\n",
        "  prefix =      input_path.split(bucket)[1][1:-1]\n",
        "  #print(prefix)\n",
        "\n",
        "  #print(blob.name.split('/')[2])\n",
        "\n",
        "  # # Crea un dizionario vuoto richiamato e un contatore cnt inizializzato a zero.\n",
        "  out = {}\n",
        "  cnt = 0\n",
        "\n",
        "  if len(list(client.list_blobs(bucket,prefix=prefix)))>0:\n",
        "\n",
        "    for blob in client.list_blobs(bucket,prefix=prefix):\n",
        "      #print(blob.name)\n",
        "\n",
        "      try:\n",
        "        out['file_name_pipeline_code_info'] = blob.name.split('/')[2]\n",
        "      except: out['file_name_pipeline_code_info'] = 'Missing File'\n",
        "\n",
        "      try:\n",
        "        out['file_name_pipeline_name_info'] = blob.name.split('/')[1]\n",
        "      except: out['file_name_pipeline_name_info'] = 'Missing File'\n",
        "\n",
        "      try: \n",
        "        out['file_name_pipeline_windowframe_info'] = blob.name.split('/')[5].split('__')[2]\n",
        "      except: out['file_name_pipeline_windowframe_info'] = 'Missing File'\n",
        "\n",
        "      if cnt > 0 : break\n",
        "\n",
        "      cnt = +1\n",
        "\n",
        "    \n",
        "  else:\n",
        "    out['file_name_pipeline_code_info'] = 'Missing File'\n",
        "    out['file_name_pipeline_name_info'] = 'Missing File'\n",
        "    out['file_name_pipeline_windowframe_info'] = 'Missing File'\n",
        "\n",
        "  return out\n",
        "\n",
        "#generate full list of days for the period covered \n",
        "\n",
        "#from datetime import datetime, date, timedelta\n",
        "\n",
        "def generate_list(period_covered,load_time_frame):\n",
        "\n",
        "  year_s = int(period_covered[0][0:4])\n",
        "  year_e = int(period_covered[1][0:4])\n",
        "\n",
        "  month_s = int(period_covered[0][4:6])\n",
        "  month_e = int(period_covered[1][4:6])\n",
        "\n",
        "  if load_time_frame == 'dd':\n",
        "\n",
        "    day_s = int(period_covered[0][6:8])\n",
        "    day_e = int(period_covered[1][6:8])\n",
        "    freq = 'd'\n",
        "\n",
        "  elif load_time_frame == 'mm':\n",
        "\n",
        "    day_s =  1\n",
        "    day_e =  1\n",
        "    freq = 'm'\n",
        "\n",
        "  start = date(year_s, month_s, day_s)  \n",
        "  end   = date(year_e, month_e, day_e) \n",
        "  \n",
        "  time_list = []\n",
        "\n",
        "  if load_time_frame == 'mm':\n",
        "    \n",
        "    for item in pd.date_range(start,end,freq=freq).strftime('%Y%m%d').to_list():\n",
        "\n",
        "      time_list.append(item[0:6])\n",
        "  \n",
        "  elif load_time_frame == 'dd':\n",
        "\n",
        "    time_list = pd.date_range(start,end,freq=freq).strftime('%Y%m%d').to_list() \n",
        "\n",
        "  \n",
        "  return time_list\n",
        "\n",
        "# #select the oldest and newest file -> from filename\n",
        "\n",
        "def get_file_name_pipeline_history_min_and_max(project_id, file_path):\n",
        "\n",
        "  client = storage.Client(project_id)\n",
        "\n",
        "  bucket_name = file_path.split('/')[2]\n",
        "  prefix = file_path.split(bucket_name)[1][1:-1]\n",
        "  \n",
        "  blobs =[]\n",
        "\n",
        "  pattern = re.compile(r'\\d\\d\\d\\d')\n",
        "  \n",
        "  for blob in client.list_blobs(bucket_name, prefix=prefix):\n",
        "    if '90_sprinklr' in blob.name:\n",
        "      if bool(pattern.match(blob.name.split('/')[4])):\n",
        "        timestamp= None\n",
        "        filename = blob.name.split('/')[-1]\n",
        "        if '__' in filename:\n",
        "          timestamp = filename.split('__')[-1].replace('.json','')\n",
        "          blobs.append([timestamp])\n",
        "          if '_' in filename:\n",
        "            timestamp = filename.split('_')[-1].replace('.json','').replace('engagement','')\n",
        "            blobs.append([timestamp])\n",
        "    else:\n",
        "      filename = blob.name.split('/')[-1]\n",
        "    if '__crm_master_ID__' in filename and filename.endswith('.csv'):\n",
        "      timestamp=filename.split('__')[-2]\n",
        "      blobs.append([timestamp])\n",
        "    elif '__mm__' in filename or '__dd__' in filename and (filename.endswith('.csv') or filename.endswith('.tsv') or filename.endswith('.json')):\n",
        "          if filename.endswith('.csv'):\n",
        "            timestamp = filename.split('__')[-1].replace('.csv', '')\n",
        "            blobs.append([timestamp])\n",
        "          elif filename.endswith('.tsv'):\n",
        "              timestamp = filename.split('__')[-1].replace('.tsv', '')\n",
        "              blobs.append([timestamp])\n",
        "          else:\n",
        "              timestamp = filename.split('__')[-1].replace('.json', '')\n",
        "              blobs.append([timestamp])\n",
        "    elif '__' in filename and (filename.endswith('.csv') or filename.endswith('.parquet')):\n",
        "          if filename.endswith('.csv'):\n",
        "            timestamp = filename.split('__')[-2]\n",
        "            blobs.append([timestamp])\n",
        "          else:\n",
        "              timestamp = filename.split('__')[-2].replace('-','')\n",
        "              blobs.append([timestamp])\n",
        "    else:\n",
        "      if '__' in filename:\n",
        "        timestamp = filename.split('__')[-1]\n",
        "        blobs.append([timestamp])\n",
        "\n",
        "  if len(blobs) > 0:\n",
        "    newest = sorted(blobs, key=lambda tup: tup[0])[-1]\n",
        "    oldest = sorted(blobs, key=lambda tup: tup[0])[0]\n",
        "  else:\n",
        "    newest = 'No files found'\n",
        "    oldest = 'No files found'\n",
        "\n",
        "  return oldest, newest, blobs\n",
        "  \n",
        "\n",
        "def check_missing_days(list1,list2):\n",
        "  #function that checks if all days are present \n",
        "  #list1 contains the set that needs to be checked\n",
        "  #list2 contains the whole list\n",
        "\n",
        "  missings = []\n",
        "\n",
        "  for item in list2:\n",
        "\n",
        "    if item in list1:\n",
        "      pass\n",
        "    else:\n",
        "      missings.append(item)\n",
        "\n",
        "  return missings\n",
        "\n",
        "def get_warning(path, freq_agg):\n",
        "  today = datetime.now(timezone.utc)\n",
        "\n",
        "  bucket_name = path.split('/')[2]\n",
        "  prefix  = path.split(bucket_name)[1][1:-1]\n",
        "\n",
        "  storage_client = storage.Client()\n",
        "  bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "  l = []\n",
        "  l2 = []\n",
        "\n",
        "  for directory in list_directories(bucket_name,prefix):\n",
        "    #print(directory)\n",
        "    l.append(directory)\n",
        "\n",
        "  blob_last_directory=max(l)\n",
        "  #print(blob_last_directory)\n",
        "\n",
        "  files = bucket.list_blobs()    \n",
        "  fileList = [file.name for file in files if blob_last_directory in file.name]\n",
        "\n",
        "  for file in fileList:\n",
        "    blob = bucket.get_blob(file)\n",
        "    l2.append(blob.updated)\n",
        "    #print(blob.updated)\n",
        "  \n",
        "  blob_last_mod=max(l2)\n",
        "  #print(blob_last_mod)\n",
        "\n",
        "  if freq_agg=='dd':\n",
        "    if (today - blob_last_mod).days < 1:\n",
        "      return 'green'\n",
        "    elif (today - blob_last_mod).days >= 1 and (today - blob_last_mod).days <= 3:\n",
        "      return 'yellow'\n",
        "    else:\n",
        "      return 'red'\n",
        "      \n",
        "  if freq_agg=='ww':\n",
        "    if (today - blob_last_mod).days < 7:\n",
        "      return 'green'\n",
        "    elif (today - blob_last_mod).days >= 7 and (today - blob_last_mod).days <= 10:\n",
        "      return 'yellow'\n",
        "    else:\n",
        "      return 'red'\n",
        "  else:\n",
        "    if (today - blob_last_mod).days < 30:\n",
        "      return 'green'\n",
        "    elif (today - blob_last_mod).days >= 30 and (today - blob_last_mod).days <= 45:\n",
        "      return 'yellow'\n",
        "    else:\n",
        "      return 'red'\n",
        "  \n",
        "def populate_9898(df,df_9898,i,columns_list,project_id):\n",
        "\n",
        "  #print the filepath analised\n",
        "  print(\"Computing row : \" + str(i) + \" of \" + str(df.shape[0]))\n",
        "  #print(df.loc[i,'landing_zone_path'])\n",
        "  print(\"landing zone path: \" + df.loc[i,'landing_zone_path'])\n",
        "\n",
        "  #fill the date\n",
        "\n",
        "  df_9898.loc[i,'analysis_date'] = datetime.now().strftime('%Y-%m-%d')\n",
        "  df_9898.loc[i,'analysis_date_time'] = datetime.now()\n",
        "\n",
        "  #add columns from the 9891\n",
        "  for col in columns_list:\n",
        "\n",
        "    df_9898.loc[i,col] = df.loc[i,col]\n",
        "\n",
        "  bucket = df.loc[i,'landing_zone_path'].split('/')[2]\n",
        "  bucket_meta = bucket_metadata(bucket,project_id)\n",
        "  \n",
        "  #add bucket metadata\n",
        "  for key in bucket_meta:\n",
        "    df_9898.loc[i,key] = bucket_meta[key] \n",
        "\n",
        "  #check that the policy and path info\n",
        "  df_9898.loc[i,'file_path_policy_respected'] = file_path_policy_respected_check(df_9898.loc[i,'landing_zone_path'])\n",
        "  df_9898.loc[i,'file_path_source_system'] = df_9898.loc[i,'landing_zone_path'].split('/')[5]\n",
        "  df_9898.loc[i,'file_path_pipeline_code'] = df_9898.loc[i,'landing_zone_path'].split('/')[4]\n",
        "\n",
        "\n",
        "  #extract bucket and prefix for a given landing_zone_path\n",
        "  bucket = df_9898.loc[i,'landing_zone_path'].split('/')[2]\n",
        "  prefix  = df_9898.loc[i,'landing_zone_path'].split(bucket)[1][1:-4]\n",
        "  #count the number of v1 v2 v3 \n",
        "  #print(bucket)\n",
        "  #print(prefix)\n",
        "  print(f\"List directories : {list_directories(bucket,prefix)}\")\n",
        "\n",
        "  df_9898.loc[i,'file_path_pipeline_code_distinct_versions'] = len(list_directories(bucket,prefix))\n",
        "\n",
        "  #compute the size of each versioning of each folder\n",
        "  \n",
        "  versions = file_path_pipeline_code_distinct_versions_check(df_9898.loc[i,'landing_zone_path'])\n",
        "\n",
        "  df_9898.loc[i,'file_path_pipeline_code_distinct_versions_sizes'] = str(versions)\n",
        "  df_9898.loc[i,'file_path_pipeline_code_checked_version'] = str(versions[-1][1])\n",
        "\n",
        "  #compute the number of files in the landing zone path\n",
        "\n",
        "  df_9898.loc[i,'file_number_in_landing_zone_path'] = get_file_number_in_landing_zone_path(df_9898.loc[i,'landing_zone_path'],)\n",
        " \n",
        "  df_9898.loc[i,'file_size_in_landing_zone_path'] = get_file_size_in_landing_zone_path(df_9898.loc[i,'landing_zone_path'])\n",
        "\n",
        "  #get oldest and most recent file in folder\n",
        "\n",
        "  new = get_oldest_and_newest_file(df_9898.loc[i,'landing_zone_path'],project_id)[0][1]\n",
        "  old = get_oldest_and_newest_file(df_9898.loc[i,'landing_zone_path'],project_id)[1][1]\n",
        "\n",
        "  # df_9898.loc[i,'file_oldest_creation_datetime'] = old\n",
        "  # df_9898.loc[i,'file_newest_creation_datetime'] = new\n",
        "\n",
        "  df_9898.loc[i,'file_oldest_creation_datetime'] = new\n",
        "  df_9898.loc[i,'file_newest_creation_datetime'] = old\n",
        "\n",
        "  #check policy of the file\n",
        "  df_9898.loc[i,'file_name_policy_respected']  = is_file_policy_respected(df_9898.loc[i,'landing_zone_path'],project_id,bucket,prefix)\n",
        "\n",
        "  #extract file infos\n",
        "\n",
        "  files_info = extract_file_info(df_9898.loc[i,'landing_zone_path'],project_id,bucket)\n",
        "\n",
        "  df_9898.loc[i,'file_name_pipeline_code_info']        =  files_info['file_name_pipeline_code_info']\n",
        "  df_9898.loc[i,'file_name_pipeline_name_info']        =  files_info['file_name_pipeline_name_info']\n",
        "  df_9898.loc[i,'file_name_pipeline_windowframe_info'] =  files_info['file_name_pipeline_windowframe_info']\n",
        "\n",
        "  #get first and latest date from filename\n",
        "\n",
        "  old,new,list_day = get_file_name_pipeline_history_min_and_max(project_id,df_9898.loc[i,'landing_zone_path'])\n",
        "  period_covered = [old[0],new[0]]\n",
        "\n",
        "  df_9898.loc[i,'file_name_pipeline_history_min_info'] = old[0]\n",
        "  df_9898.loc[i,'file_name_pipeline_history_max_info'] = new[0]\n",
        "  df_9898.loc[i,'files_period_covered'] = str(period_covered)\n",
        "\n",
        "  #get the complete list of days\n",
        "  try : \n",
        "    complete_list_days = generate_list(period_covered,df_9898.loc[i,'file_name_pipeline_windowframe_info'])\n",
        "    missings = check_missing_days(np.asarray(list_day)[:,0],complete_list_days)\n",
        "\n",
        "    if len(missings) == 0 : df_9898.loc[i,'files_period_missing_between_min_max'] = 'No missing days'\n",
        "    else: df_9898.loc[i,'files_period_missing_between_min_max'] = str(missings)\n",
        "  except:\n",
        "    print(\"Missing Files, step skipped\")\n",
        "  \n",
        "  #get_warning_status\n",
        "  try: df_9898.loc[i,'warning_flag_status'] = get_warning(df_9898.loc[i,'landing_zone_path'],df_9898.loc[i,'file_name_pipeline_windowframe_info'])\n",
        "  except : df_9898.loc[i,'warning_flag_status'] = '' \n",
        "\n",
        "  return df_9898\n",
        "\n",
        "def write_to_table(client,df_9898,bq_dataset_id,table_name):\n",
        "\n",
        "    dataset_ref = client.dataset(bq_dataset_id)\n",
        "    job_config = bigquery.LoadJobConfig()\n",
        "    job_config.autodetect = False\n",
        "    job_config.write_disposition = \"WRITE_APPEND\"\n",
        "    load_job = client.load_table_from_dataframe(df_9898, dataset_ref.table(table_name), job_config=job_config, location=\"EU\")    # API request\n",
        "    print(\"Starting job {}\".format(load_job))\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def load_into_9898():\n",
        "\n",
        "    project_id = 'advanced-analytics-278408'\n",
        "    client = bigquery.Client(project=project_id)\n",
        "\n",
        "    query = '''\n",
        "        SELECT\n",
        "        *\n",
        "        FROM `advanced-analytics-278408.data__1st_layer.9891__monitoring_information_and_params`\n",
        "        WHERE is_to_be_monitored = true\n",
        "        and landing_zone = 'GCS';\n",
        "        '''\n",
        "\n",
        "    df = client.query(query).to_dataframe()\n",
        "        #df.columns\n",
        "\n",
        "    query_2 = '''\n",
        "        SELECT\n",
        "        *\n",
        "        FROM `advanced-analytics-278408.user__rubini_riccardo.9898__pipelines_target_landing_zone_metadata_ori`;\n",
        "        '''\n",
        "\n",
        "    df_9898 = client.query(query_2).to_dataframe()\n",
        "\n",
        "        #fill the first N column of df_9898 with the first N columns of 9891__monitoring_information_and_params\n",
        "        #define a list of columns to fill \n",
        "\n",
        "    columns_list = [\n",
        "                'project_id', 'dataset_id', 'table_id', 'complete_table_id',\n",
        "                'table_code', 'is_to_be_monitored', 'current_development_status',\n",
        "                'current_development_status_id', 'source_system_id',\n",
        "                'source_system_name', 'source_system_ownership_type', 'extraction_tool',\n",
        "                'extraction_script', 'extraction_is_scheduled',\n",
        "                'extraction_schedule_pattern', 'landing_zone', 'landing_zone_path',\n",
        "                'landing_zone_form', 'loading_tool', 'load_script', 'load_is_scheduled',\n",
        "                'load_schedule_pattern', 'load_form'\n",
        "        ]\n",
        "\n",
        "        #loop over the columns to fill \n",
        "\n",
        "    #drop and recreate table \n",
        "    table_id = 'advanced-analytics-278408.user__rubini_riccardo.9898__pipelines_target_landing_zone_metadata_test'\n",
        "    client.delete_table(table_id, not_found_ok=True)  # Make an API request.\n",
        "    print(\"Deleted table '{}'.\".format(table_id))\n",
        "\n",
        "\n",
        "        #for i in range(20):\n",
        "    for i in range(df.shape[0]):\n",
        "\n",
        "            \n",
        "                df_9898_ = populate_9898(df,df_9898,i,columns_list,project_id)\n",
        "                pd.concat([df_9898, df_9898_])\n",
        "                print(df_9898_.iloc[[i]])\n",
        "                \n",
        "                \n",
        "                try : \n",
        "                  write_to_table(client,df_9898_.iloc[[i]],bq_dataset_id='user__rubini_riccardo',table_name='9898__pipelines_target_landing_zone_metadata_test')\n",
        "                  print(f'Writing Row {i}')\n",
        "\n",
        "                except : print(f'Failed to write Row {i}')\n",
        "\n",
        "    df_final = df_9898_.astype(str)\n",
        "\n",
        "    pdbq.to_gbq(df_final, \n",
        "                'user__rubini_riccardo.9898__pipelines_target_landing_zone_metadata_test_backup', \n",
        "                    project_id, if_exists='append') "
      ]
    }
  ]
}